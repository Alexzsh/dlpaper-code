{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 简介\n",
    "文本分析和NLP(Natural Language Processing，自然语言处理)是现代人工智能系统不 可分割的一部分。计算机擅长于用有限的多样性来理解结构死板的数据。然而，当我们用计算 机处理非结构化的自由文本时，就会变得很困难。开发NLP应用程序是一种挑战，因为计算机 很难理解隐含的概念，而且语言交流方式也有很多细微的差异。这些差异的形式可以是方言、 语境、俚语等。  \n",
    "为了解决这个问题，基于机器学习的NLP应运而生。这些算法检测文本数据的模式，以便可 以从中得到了解。人工智能公司大量地使用了NLP和文本分析来推送相关结果。NLP最常用的领 域包括搜索引擎、情感分析、主题建模、词性标注、实体识别等。NLP的目标是开发出一组算法， 以便可以用简单的英文和计算机交流。如果这一目标实现，将不再需要程序设计语言来命令计算 机执行指令。这一章将主要介绍文本分析，以及如何从文本数据中提取有意义的信息。我们将大 量用到Python中的NLTK(Natural Language Toolkit)包。在进行接下来的学习之前，先确保你已 经安装了NLTK，安装步骤可以参考http://www.nltk.org/install.html 你还需要安装NLTK数据，这些数据中包含很多语料和训练模型，这也是文本分析不可分割的部分，安装步骤可以参考http://www.nltk.org/data.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 用标记解析的方法预处理数据\n",
    "标记解析是将文本分割成一组有意义的片段的过程。这些片段被称作标记，例如可以将一段 文字分割成单词或者句子。根据手头的任务需要，可以自定义将输入的文本分割成有意义的标记。 接下来介绍如何实现这样的标记解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Are you curious about tokenization? Let's see how it works! We need to analyze \\\n",
    "a couple of sentences with punctuations to see it in action.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokenizer is  ['Are you curious about tokenization?', \"Let's see how it works!\", 'We need to analyze a couple of sentences with punctuations to see it in action.']\n"
     ]
    }
   ],
   "source": [
    "#提取句子\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize_list=sent_tokenize(text)\n",
    "print('Sentence tokenizer is ' , sent_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokenizer is  ['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'s\", 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n"
     ]
    }
   ],
   "source": [
    "#提取单词\n",
    "from nltk.tokenize import  word_tokenize\n",
    "word_tokenize_list=word_tokenize(text)\n",
    "print('Word tokenizer is ',word_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punkt word list is  ['Are you curious about tokenization?', \"Let's see how it works!\", 'We need to analyze a couple of sentences with punctuations to see it in action.']\n"
     ]
    }
   ],
   "source": [
    "#使用punktword根据标点分割文本 句中标点做忽略\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "punkt_word_list=PunktSentenceTokenizer()\n",
    "print('Punkt word list is ',punkt_word_list.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize_list is  ['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'\", 's', 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n"
     ]
    }
   ],
   "source": [
    "#可以将标点 符号单独保留到不同句子标记\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_punkt_list=WordPunctTokenizer()\n",
    "print('word_tokenize_list is ',word_punkt_list.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3提取文本数据词干\n",
    "处理文本文档时，可能会碰到单词的不同形式。以单词“play”为例，这个单词可能以各种 形式出现，例如“play”“plays”“player”“playing”等，这些是具有同样含义的单词家族。在文 本分析中，提取这些单词的原形非常有用，它有助于我们提取一些统计信息来分析整个文本。词 干提取的目标是将不同词形的单词都变成其原形。词干提取使用启发式处理方法截取单词的尾 部，以提取单词的原形。接下来介绍如何在Python中完成词干提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            word          PORTER       LANCASTER        SNOWBALL\n",
      "           table            tabl            tabl            tabl\n",
      "        probably         probabl            prob         probabl\n",
      "          wolves            wolv            wolv            wolv\n",
      "         playing            play            play            play\n",
      "              is              is              is              is\n",
      "             dog             dog             dog             dog\n",
      "             the             the             the             the\n",
      "         beaches           beach           beach           beach\n",
      "        grounded          ground          ground          ground\n",
      "          dreamt          dreamt          dreamt          dreamt\n",
      "        envision           envis           envid           envis\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "以上3种词干提取算法的本质目标都是提取出词干，消除词形的影响。它们的不同之处在于\n",
    "操作的严格程度不同。观察输出结果可以看到，Lancaster词干提取器比其他两个词干提取器更严格。\n",
    "就严格程度来说，Porter词干提取器是最宽松的，而Lancaster词干提取器是最严格的。\n",
    "从 Lancaster词干提取器得到的词干往往比较模糊，难以理解。\n",
    "Lancaster词干提取算法的速度很快， 但是它会减少单词的很大部分，因此通常会选择Snowball词干提取器。\n",
    "'''\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "words=['table', 'probably', 'wolves', 'playing', 'is','dog', 'the', 'beaches', 'grounded', 'dreamt', 'envision']\\\n",
    "\n",
    "stemmers=['PORTER','LANCASTER','SNOWBALL']\n",
    "stem_porter=PorterStemmer()\n",
    "stem_lancaster=LancasterStemmer()\n",
    "stem_snowball=SnowballStemmer('english')\n",
    "print(('{:>16}'*(len(stemmers)+1)).format('word',*stemmers))\n",
    "for word in words:\n",
    "    stemmed_words=[stem_porter.stem(word),stem_lancaster.stem(word),stem_snowball.stem(word)]\n",
    "    print(('{:>16}'*(len(stemmers)+1)).format(word,*stemmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 用词形还原的方法还原文本的基本形式\n",
    "词形还原的目标也是将单词转换为其原形，但它是一个更结构化的方法。在前一节中，可以 看到用词根还原得到的单词原形并不是有意义的，例如单词“wolves”被还原成“wolv”，还原 出的单词根本不是一个真实的单词。词形还原通过对单词进行词汇和语法分析来实现，因此可以 圆满解决这一问题。词形还原变形词的结尾，例如“ing”或“ed”，然后返回单词的原形形式， 这个原形也就是词根(lemma)。如果对单词“wolves”做词根还原，可以得到“wolf”的输出。 输出结果取决于标记是一个动词还是一个名词。下面看看如何做词形还原。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "\n",
      "                 WORD     NOUN LEMMATIZER     VERB LEMMATIZER \n",
      "\n",
      "               table               table               table\n",
      "            probably            probably            probably\n",
      "              wolves                wolf              wolves\n",
      "             playing             playing                play\n",
      "                  is                  is                  be\n",
      "                 dog                 dog                 dog\n",
      "                 the                 the                 the\n",
      "             beaches               beach               beach\n",
      "            grounded            grounded              ground\n",
      "              dreamt              dreamt               dream\n",
      "            envision            envision            envision\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizers = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
    "lemmatizer_wordnet = WordNetLemmatizer()\n",
    "formatted_row = '{:>20}' * (len(lemmatizers) + 1)\n",
    "print ('\\n', formatted_row.format('WORD', *lemmatizers), '\\n')\n",
    "for word in words:\n",
    "    lemmatized_words = [lemmatizer_wordnet.lemmatize(word, pos='n'),\n",
    "               lemmatizer_wordnet.lemmatize(word, pos='v')]\n",
    "    print (formatted_row.format(word, *lemmatized_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 用分块的方法划分文本\n",
    "分块是指基于任意随机条件将输入文本分割成块。与标记解析不同的是，分块没有条件约束， 分块的结果不需要有实际意义。分块在文本分析中经常使用。当处理非常大的文本文档时，就需 要将文本进行分块，以便进行下一步分析。在这一节中，将输入文本分成若干块，每块都包含固 定数目的单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/user/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "Number of text chunks = 6\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "import numpy as np\n",
    "from nltk.corpus import brown\n",
    "def splitter(data,num_words):\n",
    "    words=data.split(' ')\n",
    "    output=[]\n",
    "    cur_words=[]\n",
    "    cur_count=0\n",
    "    for word in words:\n",
    "        cur_words.append(word)\n",
    "        cur_count += 1\n",
    "        if cur_count == num_words:\n",
    "            output.append(' '.join(cur_words))\n",
    "            cur_words = []\n",
    "            cur_count = 0\n",
    "    output.append(' '.join(cur_words) )\n",
    "    return output    \n",
    "data = ' '.join(brown.words()[:10000])\n",
    "num_words = 1700\n",
    "chunks = []\n",
    "counter = 0\n",
    "text_chunks = splitter(data, num_words)\n",
    "print (\"Number of text chunks =\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 创建词袋模型\n",
    "如果需要处理包含数百万单词的文本文档，需要将其转化成某种数值表示形式，以便让机器 用这些数据来学习算法。这些算法需要数值数据，以便可以对这些数据进行分析，并输出有用的 信息。这里需要用到词袋(bag-of-words)。词袋是从所有文档的所有单词中学习词汇的模型。学 习之后，词袋通过构建文档中所有单词的直方图来对每篇文档进行建模。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab is \n",
      " ['about' 'after' 'against' 'aid' 'all' 'also' 'an' 'and' 'are' 'as' 'at'\n",
      " 'be' 'been' 'before' 'but' 'by' 'committee' 'congress' 'did' 'each'\n",
      " 'education' 'first' 'for' 'from' 'general' 'had' 'has' 'have' 'he'\n",
      " 'health' 'his' 'house' 'in' 'increase' 'is' 'it' 'last' 'made' 'make'\n",
      " 'may' 'more' 'no' 'not' 'of' 'on' 'one' 'only' 'or' 'other' 'out' 'over'\n",
      " 'pay' 'program' 'proposed' 'said' 'similar' 'state' 'such' 'take' 'than'\n",
      " 'that' 'the' 'them' 'there' 'they' 'this' 'time' 'to' 'two' 'under' 'up'\n",
      " 'was' 'were' 'what' 'which' 'who' 'will' 'with' 'would' 'year' 'years']\n",
      "Document team martrix is \n",
      "        word     Chunk-0     Chunk-1     Chunk-2     Chunk-3     Chunk-4\n",
      "       about           1           1           1           1           3\n",
      "       after           2           3           2           1           3\n",
      "     against           1           2           2           1           1\n",
      "         aid           1           1           1           3           5\n",
      "         all           2           2           5           2           1\n",
      "        also           3           3           3           4           3\n",
      "          an           5           7           5           7          10\n",
      "         and          34          27          36          36          41\n",
      "         are           5           3           6           3           2\n",
      "          as          13           4          14          18           4\n",
      "          at           5           7           9           3           6\n",
      "          be          20          14           7          10          18\n",
      "        been           7           1           6          15           5\n",
      "      before           2           2           1           1           2\n",
      "         but           3           3           2           9           5\n",
      "          by           8          22          15          14          12\n",
      "   committee           2          10           3           1           7\n",
      "    congress           1           1           3           3           1\n",
      "         did           2           1           1           2           2\n",
      "        each           1           1           4           3           1\n",
      "   education           3           2           3           1           1\n",
      "       first           4           1           4           6           3\n",
      "         for          22          19          24          27          20\n",
      "        from           4           5           6           5           5\n",
      "     general           2           2           2           3           6\n",
      "         had           3           2           7           2           6\n",
      "         has          10           2           5          20          11\n",
      "        have           4           4           4           7           5\n",
      "          he           4          13          12          13          29\n",
      "      health           1           1           2           6           1\n",
      "         his          10           6           9           3           7\n",
      "       house           5           7           4           4           2\n",
      "          in          38          27          37          49          45\n",
      "    increase           3           1           1           4           1\n",
      "          is          12           9          12          14           8\n",
      "          it          18          16           5           6           9\n",
      "        last           1           1           5           4           2\n",
      "        made           1           1           7           4           3\n",
      "        make           3           2           1           1           1\n",
      "         may           1           1           2           2           1\n",
      "        more           3           5           4           6           7\n",
      "          no           4           1           1           7           3\n",
      "         not           5           6           3          14           7\n",
      "          of          61          69          76          56          53\n",
      "          on          10          18          14          13          13\n",
      "         one           4           5           3           4           9\n",
      "        only           1           1           1           3           2\n",
      "          or           4           4           5           5           4\n",
      "       other           2           6           7           1           3\n",
      "         out           3           3           3           4           1\n",
      "        over           1           1           5           1           2\n",
      "         pay           2           3           5           4           1\n",
      "     program           2           1           4           4           5\n",
      "    proposed           2           2           1           1           1\n",
      "        said          20          15          11           9          21\n",
      "     similar           1           1           2           1           2\n",
      "       state          12           9           5           5           7\n",
      "        such           2           3           2           4           2\n",
      "        take           2           2           2           2           2\n",
      "        than           2           2           3           5           4\n",
      "        that          27          12          12          17          31\n",
      "         the         143         116         132         136         148\n",
      "        them           2           2           2           3           2\n",
      "       there           9           4           2           6           6\n",
      "        they           3           2           2           7           2\n",
      "        this           8           5           8           9           7\n",
      "        time           2           1           2           3          11\n",
      "          to          50          54          46          49          66\n",
      "         two           3           3           4           1           1\n",
      "       under           3           3           5           3           1\n",
      "          up           2           1           6           5           5\n",
      "         was          13          16          11           6          14\n",
      "        were           2           3           4           5           3\n",
      "        what           1           1           1           1           2\n",
      "       which          13          10           2           2           3\n",
      "         who           6           5           9           4           1\n",
      "        will          14           2           5          11           4\n",
      "        with           4           6           6           9          10\n",
      "       would           8          27          15           7          23\n",
      "        year           2           4           9          10           3\n",
      "       years           1           3           2           2           3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "data=' '.join(brown.words()[:10000])\n",
    "num_words=2000\n",
    "chunks=[]\n",
    "counter=0\n",
    "text_chunks=splitter(data,num_words)\n",
    "for text in text_chunks:\n",
    "    chunk={'index':counter,'text':text}\n",
    "    chunks.append(chunk)\n",
    "    counter+=1\n",
    "vectorizer=CountVectorizer(min_df=5,max_df=.95)\n",
    "doc_term_martrix=vectorizer.fit_transform([chunk['text'] for chunk in chunks])\n",
    "vocab=np.array(vectorizer.get_feature_names())\n",
    "print('Vocab is \\n',vocab)\n",
    "\n",
    "print('Document team martrix is ')\n",
    "chunk_names=['Chunk-'+str(i) for i in range(5)]\n",
    "formatted_row='{:>12}'*(len(chunk_names)+1)\n",
    "print(formatted_row.format('word',*chunk_names))\n",
    "for word,item in zip(vocab,doc_term_martrix.T):\n",
    "    output=[str(x) for x in item.data]\n",
    "    print(formatted_row.format(word,*output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 创建文本分类器\n",
    "文本分类的目的是将文本文档分为不同的类，这是NLP中非常重要的分析手段。这里将使用 一种技术，它基于一种叫作tf-idf的统计数据，它表示词频逆文档频率(term frequency—inverse 8 document frequency)。这个统计工具有助于理解一个单词在一组文档中对某一个文档的重要性。 它可以作为特征向量来做文档分类，你可以在http://www.tfidf.com中找到更多详细介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims of training data (2968, 40605)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import  fetch_20newsgroups\n",
    "category_map = {'misc.forsale': 'Sales', 'rec.motorcycles': 'Motorcycles', 'rec.sport.baseball': 'Baseball', 'sci.crypt': 'Cryptography', 'sci.space': 'Space'}\n",
    "training_data = fetch_20newsgroups(subset='train', categories=category_map.keys(), shuffle=True, random_state=7)\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "vectorizer=CountVectorizer()\n",
    "X_train_termcounts=vectorizer.fit_transform(training_data.data)\n",
    "print('dims of training data',X_train_termcounts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: The curveballs of right handed pitchers tend to curve to the left \n",
      "Predicted category: Baseball\n",
      "\n",
      "Input: Caesar cipher is an ancient form of encryption \n",
      "Predicted category: Cryptography\n",
      "\n",
      "Input: This two-wheeler is really good on slippery roads \n",
      "Predicted category: Motorcycles\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "input_data = [\n",
    "      \"The curveballs of right handed pitchers tend to curve to the left\",\n",
    "      \"Caesar cipher is an ancient form of encryption\",\n",
    "      \"This two-wheeler is really good on slippery roads\"\n",
    "]\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_termcounts)\n",
    "classifier = MultinomialNB().fit(X_train_tfidf, training_data. target)\n",
    "X_input_termcounts = vectorizer.transform(input_data)\n",
    "X_input_tfidf = tfidf_transformer.transform(X_input_termcounts)\n",
    "predicted_categories = classifier.predict(X_input_tfidf)\n",
    "# 打印输出\n",
    "for sentence, category in zip(input_data, predicted_categories):\n",
    "    print ('\\nInput:', sentence, '\\nPredicted category:', \\\n",
    "            category_map[training_data.target_names[category]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntf-idf技术常用于信息检索领域，目的是了解文档中每个单词的重要性。如果想要识别在文档 中多次出现的单词，\\n同时，像“is”和“be”这样的普通词汇并不能真正反映内容的本质，因此仅需要提取出具有实际意义的那些单词。\\n词频越大，则表示这个词越重要，同时，如果这个词经常出现，那么这个词的词频也会增加，这两个因素互相平衡。\\n提取出每个句子的词频，然后将其 转换为特征向量，用分类器来对这些句子进行分类。\\n\\n词频(The term frequency, TF)表示一个单词在给定文档中出现的频次。\\n由于不同文档的长度不同，这些频次的直方图看起来会相差很大，因此需要将其规范化，使得这些频次可以在平等 的环境下进行对比。\\n为了实现规范化，我们用频次除以文档中所有单词的个数。逆文档频率\\n(inverse document frequency，IDF)表示给定单词的重要性。当需要计算词频时，假定所有单词 是同等重要的。\\n为了抗衡哪些经常出现的单词的频率，需要用一个系数将其权重变小。我们需要 计算文档的总数目除以该单词出现的文档数目的比值。\\n逆文档频率对该比值取对数值。\\n例如，“is”或“the”等简单的单词在各个文档中均大量出现，但是这并不意味着要用这些 词作为该篇文档的特征。\\n同时，如果一个单词仅出现一次，那这个单词也不是十分有意义。因此， 我们寻找的是那些出现了一定次数，但不太频繁以至于变成噪声的单词。\\ntf-idf值的计算可以挑选 出符合要求的单词，并且可以用于分类文档。搜索引擎经常用tf-idf工具来对搜索结果进行相关度 排序。\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tf-idf技术常用于信息检索领域，目的是了解文档中每个单词的重要性。如果想要识别在文档 中多次出现的单词，\n",
    "同时，像“is”和“be”这样的普通词汇并不能真正反映内容的本质，因此仅需要提取出具有实际意义的那些单词。\n",
    "词频越大，则表示这个词越重要，同时，如果这个词经常出现，那么这个词的词频也会增加，这两个因素互相平衡。\n",
    "提取出每个句子的词频，然后将其 转换为特征向量，用分类器来对这些句子进行分类。\n",
    "\n",
    "词频(The term frequency, TF)表示一个单词在给定文档中出现的频次。\n",
    "由于不同文档的长度不同，这些频次的直方图看起来会相差很大，因此需要将其规范化，使得这些频次可以在平等 的环境下进行对比。\n",
    "为了实现规范化，我们用频次除以文档中所有单词的个数。逆文档频率\n",
    "(inverse document frequency，IDF)表示给定单词的重要性。当需要计算词频时，假定所有单词 是同等重要的。\n",
    "为了抗衡哪些经常出现的单词的频率，需要用一个系数将其权重变小。我们需要 计算文档的总数目除以该单词出现的文档数目的比值。\n",
    "逆文档频率对该比值取对数值。\n",
    "例如，“is”或“the”等简单的单词在各个文档中均大量出现，但是这并不意味着要用这些 词作为该篇文档的特征。\n",
    "同时，如果一个单词仅出现一次，那这个单词也不是十分有意义。因此， 我们寻找的是那些出现了一定次数，但不太频繁以至于变成噪声的单词。\n",
    "tf-idf值的计算可以挑选 出符合要求的单词，并且可以用于分类文档。搜索引擎经常用tf-idf工具来对搜索结果进行相关度 排序。\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.8 识别性别\n",
    "在NLP中，通过姓名识别性别是一个很有趣的任务。这里将用到启发式方法，即姓名的最后 几个字符可以界定性别特征。例如，如果某一个名字以“la”结尾，那么它很有可能是一个女性 名字，如“Angela”或者“Layla”。另外，如果一个名字以“im”结尾，那么它很有可能是一个 男性名字，例如“Tim”或者“Jim”。确定需要用到几个字符来确定性别后，可以来做这个实验。 接下来介绍如何识别性别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /Users/user/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "Number of letters 1\n",
      "acc is  74.83333333333333%\n",
      "Leonardo ==> male\n",
      "Amy ==> female\n",
      "Sam ==> male\n",
      "Number of letters 2\n",
      "acc is  77.2%\n",
      "Leonardo ==> male\n",
      "Amy ==> female\n",
      "Sam ==> male\n",
      "Number of letters 3\n",
      "acc is  76.7%\n",
      "Leonardo ==> male\n",
      "Amy ==> female\n",
      "Sam ==> female\n",
      "Number of letters 4\n",
      "acc is  69.33333333333334%\n",
      "Leonardo ==> male\n",
      "Amy ==> female\n",
      "Sam ==> female\n"
     ]
    }
   ],
   "source": [
    "nltk.download('names')\n",
    "from nltk.corpus import names\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy as nltk_accuracy\n",
    "def gender_features(word,num_letters):\n",
    "    return {'feature':word[-num_letters:].lower()}\n",
    "labeled_names=([(name,'male') for name in names.words('male.txt')]+\n",
    "              [(name,'female') for name in names.words('female.txt')])\n",
    "np.random.seed(7)\n",
    "np.random.shuffle(labeled_names)\n",
    "input_names=['Leonardo','Amy','Sam']\n",
    "for i in range(1,5):\n",
    "    print('Number of letters',i)\n",
    "    featuresets=[(gender_features(n,i),gender) for n,gender in labeled_names]\n",
    "    train_set,test_set=featuresets[3000:],featuresets[:3000]\n",
    "    classifier=NaiveBayesClassifier.train(train_set)\n",
    "    print('acc is ',str(nltk_accuracy(classifier,test_set)*100)+'%')\n",
    "    for name in input_names:\n",
    "        print(name,'==>',classifier.classify(gender_features(name,i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析句子的情感\n",
    "情感分析是NLP最受欢迎的应用之一。情感分析是指确定一段给定的文本是积极还是消极的 10 过程。有一些场景中，我们还会将“中性”作为第三个选项。情感分析常用于发现人们对于一个 特定主题的看法。情感分析用于分析很多场景中用户的情绪，如营销活动、社交媒体、电子商务 客户等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21321321'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s='21321321'\n",
    "s[-0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
